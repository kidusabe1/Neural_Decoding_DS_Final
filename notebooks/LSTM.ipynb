{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7e49c9",
   "metadata": {},
   "source": [
    "Testbed for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f36c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n",
      "neural_data \n",
      "(28039, 58)\n",
      "pos_binned \n",
      "(28039, 2)\n",
      "rmv_nrn \n",
      "(1, 12)\n",
      "rmv_nrn_std \n",
      "(1, 0)\n",
      "No lag\n"
     ]
    }
   ],
   "source": [
    "import numpy as NP\n",
    "import pickle\n",
    "import matplotlib.pyplot as PLT\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "from Neural_Decoding.decoders import LSTMDecoder\n",
    "import sklearn\n",
    "from matplotlib import pyplot as PLT\n",
    "import keras\n",
    "keras_v1=int(keras.__version__[0])<=1\n",
    "training_range=[0, 0.5]\n",
    "valid_range=[0.5,0.65]\n",
    "testing_range=[0.65, 0.8]\n",
    "lag=0\n",
    "\n",
    "\n",
    "File=open(\"C:/Users/gerri/Coding_Folder/Data_Folder/DS_Final_Data/example_data_hc.pickle\",mode='rb')\n",
    "[neural_data,pos_binned]=pickle.load(File,)\n",
    "print(f\"neural_data \\n{NP.shape(neural_data)}\"+\"\\n\"+f\"pos_binned \\n{NP.shape(pos_binned)}\")\n",
    "\n",
    "\"\"\"\n",
    "neural_data:    When neurons Fired. num_bins x num_neurons. 1 when fired, 0 when not.\n",
    "pos_binned       :    X and Y velocities\n",
    "\"\"\"\n",
    "\n",
    "#Remove all rows of pos binned and neural data where pos binned is NaN\n",
    "rmv_time=NP.where(NP.isnan(pos_binned[:,0]) | NP.isnan(pos_binned[:,1]))\n",
    "X_kf=NP.delete(neural_data,rmv_time,0)\n",
    "pos_binned=NP.delete(pos_binned,rmv_time,0)\n",
    "\n",
    "#get velocity of positions binned\n",
    "temp=NP.diff(pos_binned,axis=0)\n",
    "vels_binned=NP.concatenate((temp,temp[-1:,:]),axis=0)\n",
    "\n",
    "#We now determine acceleration, by taking diff between each position.\n",
    "temp2=NP.diff(vels_binned,axis=0)\n",
    "acc_binned=NP.concatenate((temp2,temp2[-1:,:]),axis=0)\n",
    "\n",
    "#Y_kf is the positions binned, velocities binned, then acc_binned in numpy array.\n",
    "Y_kf=NP.concatenate((pos_binned,vels_binned,acc_binned),axis=1)\n",
    "\n",
    "#Center Y along axis=0\n",
    "# Y_mean=NP.mean(Y_kf,axis=0)\n",
    "# Y_std=NP.std(Y_kf,axis=0)\n",
    "# Y_kf=(Y_kf-Y_mean)/Y_std\n",
    "\n",
    "nd_sum=NP.nansum(X_kf,axis=0) #Total number of spikes of each neuron\n",
    "nd_mean=NP.mean(nd_sum)\n",
    "nd_std=NP.std(nd_sum)\n",
    "nd_thresh=nd_mean-nd_std*2\n",
    "\n",
    "\n",
    "rmv_nrn=NP.where(nd_sum<100) #Find neurons who have less than 100 spikes total\n",
    "rmv_nrn_std=NP.where(nd_sum<nd_thresh) #Find neurons who have less than 2 std of dataset. Gerrik\n",
    "print(f\"rmv_nrn \\n{NP.shape(rmv_nrn)}\"+\"\\n\"+f\"rmv_nrn_std \\n{NP.shape(rmv_nrn_std)}\")\n",
    "\n",
    "#Centered firing covariance.\n",
    "Firing_Covariate=NP.delete(X_kf,rmv_nrn,1)\n",
    "Firing_Covariate_G=NP.delete(X_kf,rmv_nrn,1)\n",
    "\n",
    "#If there is known lag between the two datasets.\n",
    "num_examples=X_kf.shape[0]\n",
    "if lag==0:\n",
    "    print(\"No lag\")\n",
    "elif lag<0:\n",
    "    Y_kf=Y_kf[-lag:,:]\n",
    "    X_kf=X_kf[0:num_examples+lag,:]\n",
    "elif lag>0:\n",
    "    Y_kf=Y_kf[0:num_examples-lag,:]\n",
    "    X_kf=X_kf[lag:num_examples,:]\n",
    "\n",
    "\n",
    "num_examples_kf=X_kf.shape[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2da3539",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m model_lstm=LSTMDecoder(units=\u001b[32m400\u001b[39m,dropout=\u001b[32m0\u001b[39m,num_epochs=\u001b[32m5\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#Fit model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmodel_lstm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_kf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_kf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#Get predictions\u001b[39;00m\n\u001b[32m     33\u001b[39m Y_valid_predicted_lstm=model_lstm.predict(X_kf_valid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gerri\\anaconda3\\envs\\Bar_ilan\\Lib\\site-packages\\Neural_Decoding\\decoders.py:641\u001b[39m, in \u001b[36mLSTMRegression.fit\u001b[39m\u001b[34m(self, X_train, y_train)\u001b[39m\n\u001b[32m    639\u001b[39m     model.add(LSTM(\u001b[38;5;28mself\u001b[39m.units,input_shape=(X_train.shape[\u001b[32m1\u001b[39m],X_train.shape[\u001b[32m2\u001b[39m]),dropout_W=\u001b[38;5;28mself\u001b[39m.dropout,dropout_U=\u001b[38;5;28mself\u001b[39m.dropout)) \u001b[38;5;66;03m#Within recurrent layer, include dropout\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     model.add(LSTM(\u001b[38;5;28mself\u001b[39m.units,input_shape=(X_train.shape[\u001b[32m1\u001b[39m],\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m),dropout=\u001b[38;5;28mself\u001b[39m.dropout,recurrent_dropout=\u001b[38;5;28mself\u001b[39m.dropout)) \u001b[38;5;66;03m#Within recurrent layer, include dropout\u001b[39;00m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout!=\u001b[32m0\u001b[39m: model.add(Dropout(\u001b[38;5;28mself\u001b[39m.dropout)) \u001b[38;5;66;03m#Dropout some units (recurrent layer output units)\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;66;03m#Add dense connections to output layer\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for IDX,Test_Size in enumerate([0.5,0.7,0.8]):\n",
    "    X_Train, X_Test, Y_Train, Y_Test =sklearn.model_selection.train_test_split(X_kf,Y_kf,test_size=Test_Size)\n",
    "    X_Valid, X_Test, Y_Valid, Y_Test =sklearn.model_selection.train_test_split(X_Test,Y_Test,test_size=Test_Size)\n",
    "    \n",
    "    #Need to recenter, because Simpson's paradox. The grey doesnt work, but second centering does for some reason.\n",
    "    # X_kf_train_mean=NP.nanmean(X_Train,axis=0)\n",
    "    # X_kf_train_std=NP.nanstd(X_Train,axis=0)\n",
    "    # X_kf_valid_mean=NP.nanmean(X_Train,axis=0)\n",
    "    # X_kf_valid_std=NP.nanstd(X_Train,axis=0)\n",
    "    # X_kf_test_mean=NP.nanmean(X_Train,axis=0)\n",
    "    # X_kf_test_std=NP.nanstd(X_Train,axis=0)\n",
    "    # X_Train=(X_Train-X_kf_train_mean)/X_kf_train_std\n",
    "    # X_Test=(X_Test-X_kf_test_mean)/X_kf_test_std\n",
    "    # X_Valid=(X_Valid-X_kf_valid_mean)/X_kf_valid_std\n",
    "    \n",
    "    X_kf_train_mean=NP.nanmean(X_Train,axis=0)\n",
    "    X_kf_train_std=NP.nanstd(X_Train,axis=0)\n",
    "    X_kf_train=(X_Train-X_kf_train_mean)/X_kf_train_std\n",
    "    X_kf_test=(X_Test-X_kf_train_mean)/X_kf_train_std\n",
    "    X_kf_valid=(X_Valid-X_kf_train_mean)/X_kf_train_std\n",
    "    \n",
    "    y_kf_train_mean=NP.mean(Y_Train,axis=0)\n",
    "    y_kf_train=Y_Train-y_kf_train_mean\n",
    "    y_kf_test=Y_Test-y_kf_train_mean\n",
    "    y_kf_valid=Y_Valid-y_kf_train_mean\n",
    "    \n",
    "    model_lstm=LSTMDecoder(units=400,dropout=0,num_epochs=5)\n",
    "\n",
    "    #Fit model\n",
    "    model_lstm.fit(X_kf_train,y_kf_train)\n",
    "\n",
    "    #Get predictions\n",
    "    Y_valid_predicted_lstm=model_lstm.predict(X_kf_valid)\n",
    "\n",
    "    #Get metric of fit\n",
    "    R2s_lstm=get_R2(y_kf_valid,Y_valid_predicted_lstm)\n",
    "    print('R2s:', R2s_lstm)\n",
    "    \n",
    "    R2_kf=get_R2(Y_Valid,Y_valid_predicted_lstm)\n",
    "    print('R2:',R2_kf[0:2]) #I'm just printing the R^2's of the 1st and 2nd entries that correspond to the positions\n",
    "    #Next I'll get the rho^2 (the pearson correlation squared)\n",
    "    rho_kf=get_rho(Y_Valid,Y_valid_predicted_lstm)\n",
    "    print('rho2:',rho_kf[0:2]**2) #I'm just printing the rho^2's of the 1st and 2nd entries that correspond to the positions\n",
    "    \n",
    "    PLT.subplot(3,2,IDX+1)\n",
    "    PLT.plot(Y_Valid[2000:2100,0]+y_kf_train_mean[0],'b',label=\"Raw\")\n",
    "    PLT.plot(Y_valid_predicted_lstm[2000:2100,0]+y_kf_train_mean[0],'r',label=\"Predict\")\n",
    "    PLT.title(f\"Plot predict vs true. Test size {Test_Size}.\")\n",
    "    PLT.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bar_ilan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
